{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maomaodedipan/GPU/blob/main/Assignment2_question1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dQVE_uRKWar",
        "outputId": "706b6d57-f3fe-44eb-d7ea-e3107369e1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DD2360/Assignment2/question1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkdAbwRgLNZn",
        "outputId": "5480d869-0daa-4ad9-f14c-d6a2cb6e9a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DD2360/Assignment2/question1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vectorAdd.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define DataType double\n",
        "\n",
        "// Vector addition kernel\n",
        "__global__ void vecAdd(DataType *in1, DataType *in2, DataType *out, int len) {\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (index < len) {\n",
        "    out[index] = in1[index] + in2[index];\n",
        "  }\n",
        "}\n",
        "double CPUtimer(){\n",
        "  struct timeval ti;\n",
        "  gettimeofday(&ti,NULL);\n",
        "  return ((double)ti.tv_sec + (double)ti.tv_usec * 1e-6);\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "\n",
        "  int inputLength;\n",
        "  DataType *hostInput1;\n",
        "  DataType *hostInput2;\n",
        "  DataType *hostOutput;\n",
        "  DataType *resultRef;\n",
        "  DataType *deviceInput1;\n",
        "  DataType *deviceInput2;\n",
        "  DataType *deviceOutput;\n",
        "  bool flag = true;\n",
        "  double start,end,duration;\n",
        "\n",
        "  //@@ Insert code below to read in inputLength from args\n",
        "  if (argc != 2) {\n",
        "    printf(\"Wrong argument\");\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "  inputLength = atoi(argv[1]);\n",
        "  printf(\"The input length is %d\\n\", inputLength);\n",
        "\n",
        "  //@@ Insert code below to allocate Host memory for input and output\n",
        "  hostInput1 = (DataType *)malloc(inputLength * sizeof(DataType));\n",
        "  hostInput2 = (DataType *)malloc(inputLength * sizeof(DataType));\n",
        "  hostOutput = (DataType *)malloc(inputLength * sizeof(DataType));\n",
        "  resultRef = (DataType *)malloc(inputLength * sizeof(DataType));\n",
        "\n",
        "  //@@ Insert code below to initialize hostInput1 and hostInput2 to random numbers, and create reference result in CPU\n",
        "  for (int i = 0; i < inputLength; ++i) {\n",
        "    hostInput1[i] = (DataType)rand() / RAND_MAX;\n",
        "    hostInput2[i] = (DataType)rand() / RAND_MAX;\n",
        "    resultRef[i] = hostInput1[i] + hostInput2[i];\n",
        "  }\n",
        "\n",
        "  //@@ Insert code below to allocate GPU memory here\n",
        "  cudaMalloc((void **)&deviceInput1, inputLength * sizeof(DataType));\n",
        "  cudaMalloc((void **)&deviceInput2, inputLength * sizeof(DataType));\n",
        "  cudaMalloc((void **)&deviceOutput, inputLength * sizeof(DataType));\n",
        "\n",
        "  //@@ Insert code to below to Copy memory to the GPU here\n",
        "  start = CPUtimer();\n",
        "  cudaMemcpy(deviceInput1, hostInput1, inputLength * sizeof(DataType), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(deviceInput2, hostInput2, inputLength * sizeof(DataType), cudaMemcpyHostToDevice);\n",
        "  end = CPUtimer();\n",
        "  duration = end - start;\n",
        "  printf(\"time of data copy from host to device: %f.\\n\", duration);\n",
        "\n",
        "\n",
        "  //@@ Initialize the 1D grid and block dimensions here\n",
        "  int threadsPerBlock = 256;\n",
        "  int blocksPerGrid = 1000;\n",
        "\n",
        "  //@@ Launch the GPU Kernel here\n",
        "  start = CPUtimer();\n",
        "  vecAdd<<<blocksPerGrid, threadsPerBlock>>>(deviceInput1, deviceInput2, deviceOutput, inputLength);\n",
        "  cudaDeviceSynchronize(); // Wait for the GPU to finish\n",
        "  end = CPUtimer();\n",
        "  duration = end - start;\n",
        "  printf(\"time of the CUDA kernel: %f.\\n\", duration);\n",
        "\n",
        "  //@@ Copy the GPU memory back to the CPU here\n",
        "  start = CPUtimer();\n",
        "  cudaMemcpy(hostOutput, deviceOutput, inputLength * sizeof(DataType), cudaMemcpyDeviceToHost);\n",
        "  end = CPUtimer();\n",
        "  duration = end - start;\n",
        "  printf(\"time of data copy from device to host: %f.\\n\", duration);\n",
        "\n",
        "  //@@ Insert code below to compare the output with the reference\n",
        "  for (int i = 0; i < inputLength; ++i) {\n",
        "    if (fabs(hostOutput[i] - resultRef[i]) > 1e-5) {\n",
        "      printf(\"Mismatch at index %d: Host %f, GPU %f\\n\", i, resultRef[i], hostOutput[i]);\n",
        "      flag = false;\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (flag == true){\n",
        "     printf(\"Two vectors are the same\");\n",
        "  }\n",
        "\n",
        "  //@@ Free the GPU memory here\n",
        "  cudaFree(deviceInput1);\n",
        "  cudaFree(deviceInput2);\n",
        "  cudaFree(deviceOutput);\n",
        "\n",
        "  //@@ Free the CPU memory here\n",
        "  free(hostInput1);\n",
        "  free(hostInput2);\n",
        "  free(hostOutput);\n",
        "  free(resultRef);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7h6L_IZKgSf",
        "outputId": "5455fd44-4e86-4605-d1c2-819e9281291a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vectorAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vectorAdd.cu\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKsf5BBTM30K",
        "outputId": "f03cc2b2-1d18-4ce5-d030-5d437a684c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.out  Assignment2_question1.ipynb  vectorAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f59d54ZQPe3L",
        "outputId": "a71ee243-80f3-4518-fd74-2e838ca809ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 1024\n",
            "time of data copy from host to device: 0.000326.\n",
            "time of the CUDA kernel: 0.000033.\n",
            "time of data copy from device to host: 0.000025.\n",
            "Two vectors are the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --set default --metrics sm__warps_active.avg.pct_of_peak_sustained_active ./a.out 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUYKkz6VMeca",
        "outputId": "2e1fb69b-353d-4d78-f146-d197c305d0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 1024\n",
            "==PROF== Connected to process 2985 (/content/drive/MyDrive/DD2360/Assignment2/question1/a.out)\n",
            "time of data copy from host to device: 0.000334.\n",
            "==PROF== Profiling \"vecAdd\" - 0: 0%....50%....100% - 8 passes\n",
            "time of the CUDA kernel: 0.498671.\n",
            "time of data copy from device to host: 0.000062.\n",
            "==PROF== Disconnected from process 2985\n",
            "Two vectors are the same[2985] a.out@127.0.0.1\n",
            "  vecAdd(double *, double *, double *, int), 2023-Nov-26 15:27:38, Context 1, Stream 7\n",
            "    Section: Command line profiler metrics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    sm__warps_active.avg.pct_of_peak_sustained_active                                    %                          53.44\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.84\n",
            "    SM Frequency                                                             cycle/usecond                         565.27\n",
            "    Elapsed Cycles                                                                   cycle                          3,354\n",
            "    Memory [%]                                                                           %                          12.10\n",
            "    DRAM Throughput                                                                      %                           1.04\n",
            "    Duration                                                                       usecond                           5.92\n",
            "    L1/TEX Cache Throughput                                                              %                          22.17\n",
            "    L2 Cache Throughput                                                                  %                           0.75\n",
            "    SM Active Cycles                                                                 cycle                       1,826.12\n",
            "    Compute (SM) [%]                                                                     %                          12.10\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n",
            "          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n",
            "          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       1,000\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        256,000\n",
            "    Waves Per SM                                                                                                     6.25\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                              4\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          53.44\n",
            "    Achieved Active Warps Per SM                                                      warp                          17.10\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (53.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 131070"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtluZPA5KNbc",
        "outputId": "b79beee7-7139-4e82-d5af-e91b933108cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 131070\n",
            "time of data copy from host to device: 0.000920.\n",
            "time of the CUDA kernel: 0.000081.\n",
            "time of data copy from device to host: 0.000805.\n",
            "Two vectors are the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --set default --metrics sm__warps_active.avg.pct_of_peak_sustained_active ./a.out 131070"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dDKwAzqOYpb",
        "outputId": "2914470a-8d31-4aa1-ebe9-9d01584ef153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 131070\n",
            "==PROF== Connected to process 3067 (/content/drive/MyDrive/DD2360/Assignment2/question1/a.out)\n",
            "time of data copy from host to device: 0.000977.\n",
            "==PROF== Profiling \"vecAdd\" - 0: 0%....50%....100% - 8 passes\n",
            "time of the CUDA kernel: 0.298360.\n",
            "time of data copy from device to host: 0.000936.\n",
            "==PROF== Disconnected from process 3067\n",
            "Two vectors are the same[3067] a.out@127.0.0.1\n",
            "  vecAdd(double *, double *, double *, int), 2023-Nov-26 15:27:51, Context 1, Stream 7\n",
            "    Section: Command line profiler metrics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    sm__warps_active.avg.pct_of_peak_sustained_active                                    %                          76.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.74\n",
            "    SM Frequency                                                             cycle/usecond                         552.94\n",
            "    Elapsed Cycles                                                                   cycle                          6,815\n",
            "    Memory [%]                                                                           %                          61.07\n",
            "    DRAM Throughput                                                                      %                          61.07\n",
            "    Duration                                                                       usecond                          12.32\n",
            "    L1/TEX Cache Throughput                                                              %                          30.35\n",
            "    L2 Cache Throughput                                                                  %                          31.08\n",
            "    SM Active Cycles                                                                 cycle                          5,437\n",
            "    Compute (SM) [%]                                                                     %                          24.05\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       1,000\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        256,000\n",
            "    Waves Per SM                                                                                                     6.25\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                              4\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          76.79\n",
            "    Achieved Active Warps Per SM                                                      warp                          24.57\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (76.8%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 2048"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq95fxOoAqjg",
        "outputId": "8d56ddd1-60dd-484e-fa4b-bc425874f8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 2048\n",
            "time of data copy from host to device: 0.000345.\n",
            "time of the CUDA kernel: 0.000030.\n",
            "time of data copy from device to host: 0.000032.\n",
            "Two vectors are the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 4096"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utGeARIdAuox",
        "outputId": "d69124ed-0007-4a00-c649-af501da64c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 4096\n",
            "time of data copy from host to device: 0.000375.\n",
            "time of the CUDA kernel: 0.000038.\n",
            "time of data copy from device to host: 0.000049.\n",
            "Two vectors are the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 8192"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9SRkzhHAuxj",
        "outputId": "080137dc-e825-4937-abac-d6d466e42267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 8192\n",
            "time of data copy from host to device: 0.000349.\n",
            "time of the CUDA kernel: 0.000047.\n",
            "time of data copy from device to host: 0.000076.\n",
            "Two vectors are the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 16384"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGmUSz5GAu4_",
        "outputId": "b50bffd0-d3b0-4ac9-acf5-c671482fa944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 16384\n",
            "time of data copy from host to device: 0.000359.\n",
            "time of the CUDA kernel: 0.000035.\n",
            "time of data copy from device to host: 0.000121.\n",
            "Two vectors are the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 32768"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNOSZcwHAu9z",
        "outputId": "a2aefa86-bae7-4da5-81c6-fb8d3d0f2974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 32768\n",
            "time of data copy from host to device: 0.000441.\n",
            "time of the CUDA kernel: 0.000037.\n",
            "time of data copy from device to host: 0.000239.\n",
            "Two vectors are the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 65536"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78GteNrGAvBE",
        "outputId": "7333d7f2-b9b7-40c5-9fb9-4a50e533e9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 65536\n",
            "time of data copy from host to device: 0.000619.\n",
            "time of the CUDA kernel: 0.000040.\n",
            "time of data copy from device to host: 0.000438.\n",
            "Two vectors are the same"
          ]
        }
      ]
    }
  ]
}