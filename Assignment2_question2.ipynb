{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maomaodedipan/GPU/blob/main/Assignment2_question2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2nA1oH9PeG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea0df9a-6c89-4b32-8b69-741f793bea67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DD2360/Assignment2/question2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH0tBHNvQKv7",
        "outputId": "6e8cf537-241a-499e-8fc3-f51313abec19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DD2360/Assignment2/question2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vectorMultiplication.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define DataType double\n",
        "\n",
        "void initializeMatrix(DataType *matrix, int rows, int columns) {\n",
        "    for (int i = 0; i < rows; ++i) {\n",
        "        for (int j = 0; j < columns; ++j) {\n",
        "            matrix[i * columns + j] = (DataType)(rand()) / RAND_MAX;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void matrixMultiplicationCPU(DataType *A, DataType *B, DataType *C, int numRowsA, int numColsA, int numColsB) {\n",
        "    for (int i = 0; i < numRowsA; ++i) {\n",
        "        for (int j = 0; j < numColsB; ++j) {\n",
        "            DataType sum = 0.0;\n",
        "            for (int k = 0; k < numColsA; ++k) {\n",
        "                sum += A[i * numColsA + k] * B[k * numColsB + j];\n",
        "            }\n",
        "            C[i * numColsB + j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "bool areMatricesEqual(DataType *matrix1, DataType *matrix2, int numRows, int numCols) {\n",
        "    for (int i = 0; i < numRows; ++i) {\n",
        "        for (int j = 0; j < numCols; ++j) {\n",
        "            if (fabs(matrix1[i * numCols + j] - matrix2[i * numCols + j]) > 1e-5 ) {\n",
        "              printf(\"matrix1: %f,matrix2: %f\",matrix1[i * numCols + j],matrix2[i * numCols + j]);\n",
        "                return false;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "__global__ void gemm(DataType *A, DataType *B, DataType *C, int numARows,\n",
        "                      int numAColumns, int numBRows, int numBColumns) {\n",
        "    //@@ Insert code to implement matrix multiplication here\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numARows && col < numBColumns) {\n",
        "        DataType sum = 0.0;\n",
        "        for (int i = 0; i < numAColumns; ++i) {\n",
        "            sum += A[row * numAColumns + i] * B[i * numBColumns + col];\n",
        "        }\n",
        "        C[row * numBColumns + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "double CPUtimer(){\n",
        "  struct timeval ti;\n",
        "  gettimeofday(&ti,NULL);\n",
        "  return ((double)ti.tv_sec + (double)ti.tv_usec * 1e-6);\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "\n",
        "    DataType *hostA;       // The A matrix\n",
        "    DataType *hostB;       // The B matrix\n",
        "    DataType *hostC;       // The output C matrix\n",
        "    DataType *resultRef;   // The reference result\n",
        "    DataType *deviceA;\n",
        "    DataType *deviceB;\n",
        "    DataType *deviceC;\n",
        "    int numARows;          // number of rows in the matrix A\n",
        "    int numAColumns;       // number of columns in the matrix A\n",
        "    int numBRows;          // number of rows in the matrix B\n",
        "    int numBColumns;       // number of columns in the matrix B\n",
        "    int numCRows;\n",
        "    int numCColumns;\n",
        "    double start,end,duration;\n",
        "\n",
        "    //@@ Insert code below to read in numARows, numAColumns, numBRows, numBColumns from args\n",
        "    if (argc != 5) {\n",
        "        printf(\"argument doesn't match\");\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    numARows = atoi(argv[1]);\n",
        "    numAColumns = atoi(argv[2]);\n",
        "    numBRows = atoi(argv[3]);\n",
        "    numBColumns = atoi(argv[4]);\n",
        "\n",
        "    if (numAColumns != numBRows) {\n",
        "        printf(\"Dimension doesn't match!\");\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    numCRows = numARows;\n",
        "    numCColumns = numBColumns;\n",
        "\n",
        "    printf(\"Input matrix dim (%d x %d) (%d x %d) (%d x %d)\\n\", numARows, numAColumns, numBRows, numBColumns, numCRows, numCColumns);\n",
        "\n",
        "    //@@ Insert code below to allocate Host memory for input and output\n",
        "    hostA = (DataType *)malloc(numARows * numAColumns * sizeof(DataType));\n",
        "    hostB = (DataType *)malloc(numBRows * numBColumns * sizeof(DataType));\n",
        "    hostC = (DataType *)malloc(numCRows * numCColumns * sizeof(DataType));\n",
        "    resultRef = (DataType *)malloc(numCRows * numCColumns * sizeof(DataType));\n",
        "\n",
        "    //@@ Insert code below to initialize hostA and hostB to random numbers, and create reference result in CPU\n",
        "    initializeMatrix(hostA, numARows, numAColumns);\n",
        "    initializeMatrix(hostB, numBRows, numBColumns);\n",
        "    matrixMultiplicationCPU(hostA, hostB, resultRef, numARows, numAColumns, numBColumns);\n",
        "\n",
        "    //@@ Insert code below to allocate GPU memory here\n",
        "    cudaMalloc((void **)&deviceA, numARows * numAColumns * sizeof(DataType));\n",
        "    cudaMalloc((void **)&deviceB, numBRows * numBColumns * sizeof(DataType));\n",
        "    cudaMalloc((void **)&deviceC, numCRows * numCColumns * sizeof(DataType));\n",
        "\n",
        "    //@@ Insert code to below to Copy memory to the GPU here\n",
        "    start = CPUtimer();\n",
        "    cudaMemcpy(deviceA, hostA, numARows * numAColumns * sizeof(DataType), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(deviceB, hostB, numBRows * numBColumns * sizeof(DataType), cudaMemcpyHostToDevice);\n",
        "    end = CPUtimer();\n",
        "    duration = end - start;\n",
        "    printf(\"copy time(H2D): %f.\\n\", duration);\n",
        "\n",
        "\n",
        "    //@@ Initialize the grid and block dimensions here\n",
        "    dim3 blockDim(64, 64); // You may adjust the block size as needed\n",
        "    dim3 gridDim((numBColumns + blockDim.x - 1) / blockDim.x, (numARows + blockDim.y - 1) / blockDim.y);\n",
        "\n",
        "    //@@ Launch the GPU Kernel here\n",
        "    start = CPUtimer();\n",
        "    gemm<<<gridDim, blockDim>>>(deviceA, deviceB, deviceC, numARows, numAColumns, numBRows, numBColumns);\n",
        "    cudaDeviceSynchronize();\n",
        "    end = CPUtimer();\n",
        "    duration = end - start;\n",
        "    printf(\"kernel time: %f.\\n\", duration);\n",
        "\n",
        "    //@@ Copy the GPU memory back to the CPU here\n",
        "    start = CPUtimer();\n",
        "    cudaMemcpy(hostC, deviceC, numCRows * numCColumns * sizeof(DataType), cudaMemcpyDeviceToHost);\n",
        "    end = CPUtimer();\n",
        "    duration = end - start;\n",
        "    printf(\"copy time(D2H): %f.\\n\", duration);\n",
        "\n",
        "    //@@ Insert code below to compare the output with the reference\n",
        "    if(areMatricesEqual(resultRef, hostC, numCRows, numCColumns)){\n",
        "      printf(\"The result is euqal\");\n",
        "    }else{\n",
        "      printf(\"The result is not euqal\");\n",
        "    }\n",
        "\n",
        "    //@@ Free the GPU memory here\n",
        "    cudaFree(deviceA);\n",
        "    cudaFree(deviceB);\n",
        "    cudaFree(deviceC);\n",
        "\n",
        "    //@@ Free the CPU memory here\n",
        "    free(hostA);\n",
        "    free(hostB);\n",
        "    free(hostC);\n",
        "    free(resultRef);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6SK9ytOSlr_",
        "outputId": "726cb8ca-0105-4e00-e79f-4764aa74de43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vectorMultiplication.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vectorMultiplication.cu\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdMbjoqeT54P",
        "outputId": "e1f82d1f-b552-4947-8277-2346e9dfabb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.out  vectorMultiplication.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 20 10 10 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y-QAFioUJFJ",
        "outputId": "6a739e4d-b7bf-406f-cd25-138da16c798d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (20 x 10) (10 x 20) (20 x 20)\n",
            "copy time(H2D): 0.000276.\n",
            "kernel time: 0.000036.\n",
            "copy time(D2H): 0.000022.\n",
            "The result is euqal"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 128 128 128 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_OKtZ9GSwIb",
        "outputId": "bb452ef2-077d-4423-fc55-172e193c6256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (128 x 128) (128 x 128) (128 x 128)\n",
            "copy time(H2D): 0.000575.\n",
            "kernel time: 0.000092.\n",
            "copy time(D2H): 0.000131.\n",
            "The result is euqal"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 511 1023 1023 4094"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ErBPJSoS7od",
        "outputId": "b2b829c9-8ab0-45cc-d0ba-000b5d518459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (511 x 1023) (1023 x 4094) (511 x 4094)\n",
            "copy time(H2D): 0.008174.\n",
            "kernel time: 0.047718.\n",
            "copy time(D2H): 0.011278.\n",
            "The result is euqal"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --set default --metrics sm__warps_active.avg.pct_of_peak_sustained_active ./a.out 511 1023 1023 4094"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ0F4ljtUa3i",
        "outputId": "79540c35-3d84-41a3-e3fd-4479fa9e81a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (511 x 1023) (1023 x 4094) (511 x 4094)\n",
            "==PROF== Connected to process 6931 (/content/drive/MyDrive/DD2360/Assignment2/question2/a.out)\n",
            "copy time(H2D): 0.008287.\n",
            "==PROF== Profiling \"gemm\" - 0: 0%....50%....100% - 8 passes\n",
            "kernel time: 0.670145.\n",
            "copy time(D2H): 0.012505.\n",
            "==PROF== Disconnected from process 6931\n",
            "The result is euqal[6931] a.out@127.0.0.1\n",
            "  gemm(double *, double *, double *, int, int, int, int), 2023-Nov-26 16:04:25, Context 1, Stream 7\n",
            "    Section: Command line profiler metrics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    sm__warps_active.avg.pct_of_peak_sustained_active                                    %                          99.18\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           5.00\n",
            "    SM Frequency                                                             cycle/usecond                         585.02\n",
            "    Elapsed Cycles                                                                   cycle                     27,876,673\n",
            "    Memory [%]                                                                           %                          25.06\n",
            "    DRAM Throughput                                                                      %                          11.46\n",
            "    Duration                                                                       msecond                          47.65\n",
            "    L1/TEX Cache Throughput                                                              %                          50.13\n",
            "    L2 Cache Throughput                                                                  %                           5.69\n",
            "    SM Active Cycles                                                                 cycle                  27,816,016.43\n",
            "    Compute (SM) [%]                                                                     %                          96.20\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing workloads in the Compute Workload Analysis section.                                        \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       8,192\n",
            "    Registers Per Thread                                                   register/thread                             64\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                      2,097,152\n",
            "    Waves Per SM                                                                                                    51.20\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                              4\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                              4\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          99.18\n",
            "    Achieved Active Warps Per SM                                                      warp                          31.74\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 16 16 16 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBfxezYpYxlN",
        "outputId": "9cfeaf62-5392-4728-c46b-3b6d7d051dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (16 x 16) (16 x 16) (16 x 16)\n",
            "copy time(H2D): 0.000324.\n",
            "kernel time: 0.000031.\n",
            "copy time(D2H): 0.000018.\n",
            "The result is euqal"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 32 32 32 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwY6wC52Y0sR",
        "outputId": "38490eb7-fa3e-4d1e-e166-e57f291cd676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (32 x 32) (32 x 32) (32 x 32)\n",
            "copy time(H2D): 0.000292.\n",
            "kernel time: 0.000036.\n",
            "copy time(D2H): 0.000023.\n",
            "The result is euqal"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 64 64 64 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qsad8xRRY1A4",
        "outputId": "8478f66c-1d91-4c43-c1ad-02be6aeebc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (64 x 64) (64 x 64) (64 x 64)\n",
            "copy time(H2D): 0.000755.\n",
            "kernel time: 0.000054.\n",
            "copy time(D2H): 0.000063.\n",
            "The result is euqal"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 256 256 256 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B1ooDY6Y1Ka",
        "outputId": "dbed9874-984f-4fd9-b127-5fb9b435965d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (256 x 256) (256 x 256) (256 x 256)\n",
            "copy time(H2D): 0.000591.\n",
            "kernel time: 0.000448.\n",
            "copy time(D2H): 0.000471.\n",
            "The result is euqal"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out 512 512 512 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiRqIlLAbEvO",
        "outputId": "b7230a4e-eb13-4729-d918-38ae2d4f070a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input matrix dim (512 x 512) (512 x 512) (512 x 512)\n",
            "copy time(H2D): 0.001356.\n",
            "kernel time: 0.000071.\n",
            "copy time(D2H): 0.001617.\n",
            "matrix1: 135.774959,matrix2: 0.000000The result is not euqal"
          ]
        }
      ]
    }
  ]
}